Goal:
Given n data points (x₁, y₁), (x₂, y₂), …, (xₙ, yₙ),
we want to fit a line:

ŷ = m*x + c

that minimizes the Mean Squared Error (MSE):

J(m, c) = (1/n) * Σ (yᵢ - (m*xᵢ + c))²

Residuals and SSE

Define residuals:

eᵢ = yᵢ - (m*xᵢ + c)

Then the sum of squared errors (SSE) is:

SSE(m, c) = Σ (yᵢ - m*xᵢ - c)²

Normal Equations

To minimize SSE, take partial derivatives and set them to zero:

∂SSE/∂m = -2 Σ xᵢ (yᵢ - mxᵢ - c) = 0
∂SSE/∂c = -2 Σ (yᵢ - mxᵢ - c) = 0

Solving these gives:

m = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²
c = ȳ - m*x̄

Or equivalently:

m = S_xy / S_xx
c = ȳ - m*x̄

Gradient Descent Approach

We can also minimize J(m, c) iteratively using gradient descent.

Partial derivatives:

∂J/∂m = (-2/n) Σ xᵢ (yᵢ - mxᵢ - c)
∂J/∂c = (-2/n) Σ (yᵢ - mxᵢ - c)

Update rules (learning rate = α):

m = m - α * (∂J/∂m)
c = c - α * (∂J/∂c)

Coefficient of Determination (R²)

To measure goodness of fit:

TSS = Σ (yᵢ - ȳ)² → Total Sum of Squares
RSS = Σ (yᵢ - ŷᵢ)² → Residual Sum of Squares
R² = 1 - (RSS / TSS)

Summary

Closed-form solution:
m = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²
c = ȳ - m*x̄

Gradient Descent: iterative method to minimize J(m, c).
R² measures how well the line explains the variance in y.