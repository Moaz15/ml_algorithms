Assumptions of Linear Regression 

2.Independence of Errors (No Autocorrelation)
Residuals (errors) from predictions should be independent ‚Äî meaning, the error from one observation should not influence another.
This mainly matters if  data is time-dependent (like time-series or sequential samples).
The error for one data point should not give you any clue about the error for the next one
If they are related ‚Äî like if one big error is often followed by another big error ‚Äî it means there‚Äôs a pattern left in the data that the model didn‚Äôt capture.
Residuals should be random and not connected. If they‚Äôre not, your model is missing something (like a time trend or seasonal effect).

The Durbin‚ÄìWatson statistic checks for autocorrelation in residuals ‚Äî basically whether the residuals are related to each other across observations.

The Durbin‚ÄìWatson statistic is calculated as:
           DW‚âà2(1‚àíœÅ)

where are the residuals from the regression.The numerator measures how much each residual differs from the previous one (a measure of change).
The denominator is the total variance in residuals.

| œÅ (autocorrelation) | DW ‚âà 2(1 ‚àí œÅ) | Interpretation                                             |
| ------------------- | ------------- | ---------------------------------------------------------- |
| œÅ = 0               | DW ‚âà 2        | No autocorrelation                                         |
| œÅ > 0               | DW < 2        | Positive autocorrelation (residuals tend to move together) |
| œÅ < 0               | DW > 2        | Negative autocorrelation (residuals alternate in sign)     |

Possible fixes:

Add lag features (previous time steps).

Use ARIMA / SARIMA models for time data.

Ensure data order isn‚Äôt chronological when it shouldn‚Äôt be.

3. Homoscedasticity (Constant Variance of Errors)
üîπ Meaning:
Homoscedasticity means that the variance of residuals (errors) is constant across all levels of the predicted values or independent variables.
In simple terms, the spread of errors should be even ‚Äî not increasing or decreasing with the predicted value.

The mistakes your regression model makes should be evenly spread across all values ‚Äî not bigger or smaller for some parts of the data.
üîπ Example:
Imagine you‚Äôre predicting income based on years of experience:
People with low experience ‚Üí model predicts well (small errors).
People with high experience ‚Üí income varies a lot (large errors).

Here, the variance of residuals changes with experience ‚Üí this is called Heteroscedasticity.
That means the variance is not constant, violating an important regression assumption.

üîπ Why Heteroscedasticity is a Problem:
Confidence intervals and p-values become unreliable.
The model‚Äôs accuracy is uneven ‚Äî it fits some parts of the data better than others.
Standard errors become biased, leading to wrong conclusions about variable significance.

üîπ Key Points:
Homoscedasticity = even spread of errors (constant variance).
Heteroscedasticity = uneven or funnel-shaped spread of errors (changing variance).
Residuals should look like random noise, evenly spread around zero for all predicted values.

Remedies: 
| **Remedy**             | **Approach**                      | **When to Use**                              |
| ---------------------- | --------------------------------- | -------------------------------------------- |
| Log / sqrt transform   | Stabilize variance                | When residual spread increases with Y        |
| Weighted Least Squares | Reweight high-error points        | When variance is proportional to predictions |
| Robust Regression      | Downweight outliers automatically | When data has noise/outliers                 |
| Add missing features   | Improve model specification       | When key variable is missing                 |
| Re-specify model       | Handle structure in data          | When groups/time cause variance shifts       |


4. Normality of Residuals

This assumption states that the residuals (errors) from linear regression model should be normally distributed ‚Äî i.e., they follow a bell-shaped curve centered around zero.
Most prediction errors should be small (near 0), and large errors should be rare.

Ensures the validity of hypothesis tests (t-test, F-test).
Gives reliable confidence intervals and p-values for coefficients.
Helps the model make statistically sound predictions.

Even if the residuals aren‚Äôt perfectly normal, linear regression predictions remain unbiased ‚Äî but inference quality (like hypothesis tests) becomes questionable.

Remedies for Non-Normal Residuals

1.Transform the Target Variable (Most Effective)
2.Remove or Cap Outliers
3.Add Missing Predictors
If residuals remain non-normal, it might mean your model is missing key explanatory features.


5 Multicollinearity (No High Correlation Between Predictors)

Multicollinearity occurs when two or more independent variables are highly correlated with each other.
This means they carry redundant information, which confuses the model while estimating coefficients.
If two predictors give almost the same information, the model gets confused about which one is actually causing the effect.
The model can still predict well, but its coefficients (Œ≤ values) become unstable and unreliable.

When features are highly correlated:

The model struggles to determine the true effect of each feature.
Coefficients become very sensitive to small data changes.
p-values and standard errors become meaningless.
You might even see wrong signs (positive/negative) for coefficients.

It‚Äôs a problem when:

You want to interpret coefficients (like in regression analysis).
You can‚Äôt tell which variable really affects the outcome.
Signs (positive/negative) or sizes of coefficients become unreliable.
You have very high correlation (close to 1 or -1) between predictors.
The model becomes unstable ‚Äî small data changes cause large swings in coefficient values.
p-values and standard errors become misleading.
It might look like no variable is significant, even though together they are.

MULTICOLLINEARITY (Mathematical Explanation)
--------------------------------------------

1. Linear Regression Model:
   y = Œ≤0 + Œ≤1X1 + Œ≤2X2 + ... + Œ≤kXk + Œµ

   - y  = dependent variable
   - X1, X2, ..., Xk = independent variables (predictors)
   - Œ≤0, Œ≤1, ..., Œ≤k = coefficients
   - Œµ  = error term

2. Matrix Form:
   y = XŒ≤ + Œµ

   - X is an n√ók matrix of predictors
   - Œ≤ is a k√ó1 vector of coefficients

   The least squares estimate is:
   Œ≤ÃÇ = (X·µÄX)‚Åª¬π X·µÄy

3. Where Multicollinearity Appears:
   - If two or more columns in X are highly correlated,
     one column can be written as a linear combination of others:
       Xj = a1X1 + a2X2 + ... + aj-1Xj-1

   - In this case, X·µÄX becomes nearly singular
     (its inverse (X·µÄX)‚Åª¬π becomes unstable or impossible to compute).

4. Why It‚Äôs a Problem:
   - The variance of Œ≤ÃÇ is:
       Var(Œ≤ÃÇ) = œÉ¬≤ (X·µÄX)‚Åª¬π

     If (X·µÄX)‚Åª¬π is large or unstable:
       ‚Üí Coefficients (Œ≤ÃÇ) change a lot with small data changes
       ‚Üí Standard errors become large
       ‚Üí p-values become unreliable

5. Measuring Multicollinearity (VIF - Variance Inflation Factor):
   For each variable Xj:
       VIFj = 1 / (1 - Rj¬≤)

   - Rj¬≤ is the R-squared from regressing Xj on all other predictors
   - If Rj¬≤ is high ‚Üí VIFj is large ‚Üí multicollinearity exists

Interpretation:
- VIF = 1      ‚Üí No correlation
- VIF = 1‚Äì5    ‚Üí Low to moderate correlation (acceptable)
- VIF > 5      ‚Üí Problematic multicollinearity
- VIF > 10     ‚Üí Severe multicollinearity (serious concern)


6. In Simple Words:
   When predictors are highly correlated,
   X·µÄX is hard to invert,
   which makes the estimated coefficients (Œ≤ÃÇ) unstable and unreliable.

| Type                     | Description                              | Invertible (X·µÄX)? | Regression Works? |
|--------------------------|------------------------------------------|-------------------|-------------------|
| Perfect Multicollinearity | Exact linear relationship                | ‚ùå No             | ‚ùå No             |
| Non-Perfect Multicollinearity | Very high (but not exact) correlation | ‚úÖ Yes            | ‚ö†Ô∏è Yes, but unreliable |

- Perfect multicollinearity ‚Üí model fails.
- Non-perfect multicollinearity ‚Üí model works, but results are unstable.

------------------------------------------------------------
| Type                     | Cause                         | Correlation | Regression Works? | Problem Severity |
|---------------------------|--------------------------------|--------------|-------------------|------------------|
| Structural (Perfect)      | Model design / dummy trap      | |r| = 1      | ‚ùå No             | üî¥ Severe        |
| Data-Based (Imperfect)    | High correlation in data       | |r| < 1      | ‚ö†Ô∏è Yes, but unstable | üü† Moderate/Severe |


- Structural multicollinearity ‚Üí caused by model setup (exact relationship)
- Data-based multicollinearity ‚Üí caused by data patterns (strong but not exact relationship)

Detection : 

| Method                     | What It Checks                          | Threshold / Warning Sign     |
|-----------------------------|------------------------------------------|------------------------------|
| Correlation Matrix          | Pairwise correlations                    | |r| > 0.8                    |
| Variance Inflation Factor   | R¬≤ among predictors                      | VIF > 5 or 10                |
| Tolerance                   | Inverse of VIF                           | < 0.1 or < 0.2               |

- Start with a correlation matrix for a quick check.
- Use VIF and Tolerance for precise measurement.
- High VIF or low Tolerance = multicollinearity problem.

HOW TO REMOVE MULTICOLLINEARITY
-------------------------------

Multicollinearity occurs when two or more independent variables are highly correlated.
It makes the regression coefficients unstable and difficult to interpret.

Here are the main methods to reduce or remove it:

------------------------------------------------------------
1. REMOVE HIGHLY CORRELATED VARIABLES
------------------------------------------------------------
- Check the correlation matrix or VIF values.
- If two variables are highly correlated (|r| > 0.8 or VIF > 10),
  remove one of them from the model.

Example:
  If ‚ÄúHeight in inches‚Äù and ‚ÄúHeight in centimeters‚Äù are both included,
  keep only one since they represent the same information.

------------------------------------------------------------
2. COMBINE VARIABLES
------------------------------------------------------------
- If two variables measure similar concepts, combine them into one.

Example:
  If you have:
    X1 = "number of purchases"
    X2 = "amount spent"
  You can create:
    X_combined = X1 * X2  (or their average)
  This reduces redundancy and keeps useful information.

------------------------------------------------------------
3. USE PRINCIPAL COMPONENT ANALYSIS (PCA)
------------------------------------------------------------
- PCA transforms correlated variables into a smaller set of
  uncorrelated components (called principal components).
- These components capture most of the information but remove correlation.

Effect:
- Eliminates multicollinearity completely.
- But the new components lose some interpretability (not original variables).

------------------------------------------------------------
4. CENTER OR STANDARDIZE VARIABLES
------------------------------------------------------------
- When multicollinearity arises due to interaction terms or polynomial terms,
  centering can help.

Example:
  Instead of using X and X¬≤ directly,
  create centered versions:
    X_centered = X - mean(X)
    X_centered¬≤ = (X - mean(X))¬≤

- This reduces correlation between X and X¬≤.

------------------------------------------------------------
5. USE REGULARIZATION METHODS
------------------------------------------------------------
- Apply regression techniques that can handle multicollinearity:
  - **Ridge Regression (L2 Regularization)**:
    Shrinks coefficients of correlated variables toward zero.
  - **Lasso Regression (L1 Regularization)**:
    Can completely eliminate some coefficients (variable selection).

Effect:
- Stabilizes the coefficients.
- Improves prediction accuracy even with correlated predictors.

------------------------------------------------------------
6. INCREASE SAMPLE SIZE
------------------------------------------------------------
- Sometimes multicollinearity occurs due to small datasets.
- Collecting more data can help make variable relationships clearer
  and reduce instability.

------------------------------------------------------------
7. DROP ONE DUMMY VARIABLE (Dummy Variable Trap)
------------------------------------------------------------
- In categorical variables represented by dummy variables,
  always drop one category to avoid perfect multicollinearity.

Example:
  For a variable "Gender" with categories Male and Female:
    Use only one dummy:
      Male = 1 if male, 0 otherwise
    (Do NOT include Female dummy + intercept together)

------------------------------------------------------------
8. SUMMARY TABLE
------------------------------------------------------------
| Method                        | When to Use                            | Effect on Model              |
|-------------------------------|----------------------------------------|------------------------------|
| Remove correlated variables   | Variables are redundant                | Simplifies model             |
| Combine variables             | Variables measure same concept         | Keeps info, reduces overlap  |
| PCA                           | Many correlated variables              | Removes correlation, less interpretable |
| Center/Standardize            | Polynomial or interaction terms        | Reduces correlation          |
| Ridge/Lasso Regression        | Prediction-focused models              | Stabilizes coefficients      |
| Increase sample size          | Small dataset                          | Reduces random correlations  |
| Drop dummy variable           | Dummy variable trap                    | Fixes perfect multicollinearity |

------------------------------------------------------------
IN SHORT:
- First, detect which variables are correlated (using VIF or correlation matrix).
- Then, try removing, combining, transforming, or regularizing them.
- Goal: Keep important information but make predictors independent enough.


