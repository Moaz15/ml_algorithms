1. What is Linear Regression?
------------------------------
Linear Regression is a supervised learning algorithm used to model the 
relationship between a continuous dependent variable (target) and one or 
more independent variables (features).

Goal: Learn weights (w) such that predicted values (ŷ) are as close as 
possible to the true values (y), minimizing the error.

2. Types of Linear Regression
-----------------------------
(a) Simple Linear Regression:
    One feature x
    Equation:
        ŷ = w0 + w1 * x

(b) Multiple Linear Regression:
    Multiple features x1, x2, ..., xn
    Equation:
        ŷ = w0 + w1 * x1 + w2 * x2 + ... + wn * xn

3. Hypothesis Function
----------------------
We express prediction as a dot product of weights and inputs:
    ŷ = w^T * x

Expanded form:
    ŷ = w0 + w1 * x1 + w2 * x2 + ... + wn * xn

Where:
    x = [1, x1, x2, ..., xn]         --> feature vector with bias
    w = [w0, w1, w2, ..., wn]        --> weight vector including bias

4. Cost Function (Mean Squared Error)
-------------------------------------
We define a cost function to measure prediction error.

Mean Squared Error (MSE):
    J(w) = (1 / 2m) * Σ (ŷᵢ - yᵢ)^2

Where:
    m      = number of training samples
    yᵢ     = true value
    ŷᵢ     = predicted value

Note: We include 1/2 for easier gradient derivation.

5. Gradient Descent
-------------------
Used to minimize the cost function J(w) by iteratively updating weights.

Update Rule:
    w_j := w_j - α * (∂J / ∂w_j)

Where:
    α      = learning rate
    ∂J/∂w_j = gradient of the cost with respect to weight w_j

Gradient of J(w):
    ∂J/∂w_j = (1 / m) * Σ (ŷᵢ - yᵢ) * x_jᵢ

6. Bias Handling
----------------
We add a column of 1s to the feature matrix X to handle the intercept term.

X (original) shape: (m, n)
X (with bias) shape: (m, n + 1)

This allows the hypothesis to be expressed as:
    ŷ = w^T * x
Including:
    w0 (bias), w1, ..., wn
    1 ,  x1 , ..., xn

7. Stopping Criteria
--------------------
Gradient Descent stops when:
    - A fixed number of iterations (epochs) is reached
    - OR change in cost J(w) between steps is below a threshold (convergence)

8. Prediction
-------------
Once weights w are learned:
    ŷ = w^T * x

Used to make predictions on new unseen data.

9. Evaluation Metric: R² Score (Coefficient of Determination)
--------------------------------------------------------------
Measures how well predictions approximate actual values.

R² = 1 - (Σ (yᵢ - ŷᵢ)^2) / (Σ (yᵢ - ȳ)^2)

Where:
    ȳ = mean of true values yᵢ

Interpretation:
    R² = 1   --> perfect prediction
    R² = 0   --> predicts mean only
    R² < 0   --> worse than mean prediction


10. Key Assumptions of Linear Regression
----------------------------------------
To use linear regression effectively, we assume:

1. Linearity:
    - The relationship between the independent variables (X) and the dependent variable (y)
      is linear.

2.
Independence of Residuals
-------------------------------------
Definition:
    Residuals (errors) from different observations should not be correlated.

Mathematical Form:
    Cov(εᵢ, εⱼ) = 0  for i ≠ j

Why Important:
    - Violating this leads to unreliable standard errors
    - Misleading model performance
    - Bias in coefficient estimates

How to Detect:
    - Durbin-Watson test
    - Residual vs. observation index plot
    - Autocorrelation Function (ACF) plot

How to Fix:
    - Use time-series models (ARIMA, Prophet)
    - Add lagged features (x_t-1, x_t-2, ...)
    - Use Generalized Least Squares (GLS)

"""


3. Homoscedasticity:
"""
Assumption: Homoscedasticity (Constant Error Variance)
-------------------------------------------------------
Definition:
    The residuals should have constant variance across all fitted values.

Mathematical Form:
    Var(ε | x) = σ²  for all x

Why Important:
    - Ensures valid confidence intervals and hypothesis tests.
    - Keeps standard errors reliable and unbiased.

Detection Methods:
    - Residual vs Predicted plot (look for patterns like funnel shape)
    - Breusch-Pagan test
    - Goldfeld-Quandt test

Fixes:
    - Transform target (log, sqrt)
    - Use Weighted Least Squares (WLS)
    - Use robust standard errors (HC1, HC3)

"""

4. Normality of Errors:

"""
Assumption: Normality of Residuals (ε ~ N(0, σ²))
-------------------------------------------------
Definition:
    Residuals should be normally distributed with mean 0 and constant variance.

Why Important:
    - Required for valid confidence intervals and p-values
    - Affects statistical tests but not predictions

Detection Methods:
    - Histogram of residuals (should be bell-shaped)
    - Q-Q plot (should follow 45° line)
    - Shapiro-Wilk test (p > 0.05 indicates normality)

Fixes:
    - Transform the target (log, sqrt, Box-Cox)
    - Remove outliers
    - Use robust models or bootstrapping

"""

5. No Multicollinearity :

"""
Assumption: No Perfect Multicollinearity
----------------------------------------
Definition:
    No feature should be a linear combination of other features.

Why Important:
    - Ensures stable and interpretable coefficient estimates
    - Prevents inflated standard errors and misleading p-values

Detection Methods:
    - Correlation matrix
    - Variance Inflation Factor (VIF)

VIF Interpretation:
    - VIF < 5: OK
    - VIF 5–10: Moderate multicollinearity
    - VIF > 10: High multicollinearity

Fixes:
    - Remove correlated features
    - Combine features (e.g., PCA)
    - Use regularization (Ridge, Lasso)

"""

