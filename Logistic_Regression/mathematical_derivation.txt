#  Logistic Regression with L2 Regularization â€“ Full Mathematical Derivation 
#  1. Problem Statement

We are solving a **binary classification** problem.  
Given:
- Input vector: x âˆˆ â„â¿
- Output label: y âˆˆ {0, 1}

Our goal is to learn a function that estimates the probability:
P(y = 1 | x)

---
## ğŸ”¢ 2. Hypothesis Function (Sigmoid)
We start with a linear function:

z = Î¸áµ€x + b
To map the real-valued output into a probability, we use the **sigmoid function**:

Ïƒ(z) = 1 / (1 + e^(âˆ’z))

So our predicted probability is:

Å· = hÎ¸(x) = Ïƒ(Î¸áµ€x + b)

---

##  3. Interpreting the Output
- Å· = P(y = 1 | x)
- 1 âˆ’ Å· = P(y = 0 | x)

---

## ğŸ“‰ 4. Loss Function: Binary Cross Entropy
We use cross-entropy to measure the mismatch between predicted probability and true label.

For one example:

L(Å·, y) = âˆ’[y log(Å·) + (1 âˆ’ y) log(1 âˆ’ Å·)]

For all m examples:

J(Î¸) = âˆ’(1/m) âˆ‘ [yáµ¢ log(Å·áµ¢) + (1 âˆ’ yáµ¢) log(1 âˆ’ Å·áµ¢)]

---

##  5. The Overfitting Problem
Without any constraint on Î¸, the model can assign **very large weights** to fit training data perfectly â€” even on noise.  
This causes **overfitting**, leading to **high variance** and poor generalization.

---

##  6. Adding L2 Regularization
To prevent large weights, we add an L2 penalty:

J(Î¸) = CrossEntropyLoss + (Î» / 2m) âˆ‘ Î¸â±¼Â²

Where:
- Î»: regularization strength
- Bias b is **not penalized**

---

##  7. Gradient Derivation with Regularization

Let:

error = Å· âˆ’ y

Then:

**Gradient of loss w.r.t. weights:**

âˆ‚J/âˆ‚Î¸ = (1/m) Xáµ€(error) + (Î»/m) Î¸

**Gradient of loss w.r.t. bias:**

âˆ‚J/âˆ‚b = (1/m) âˆ‘ (Å·áµ¢ âˆ’ yáµ¢)

---

##  8. Parameter Update Rules (Gradient Descent)

Î¸ := Î¸ âˆ’ Î± [(1/m) Xáµ€(error) + (Î»/m) Î¸]  
b := b âˆ’ Î± (1/m) âˆ‘ (Å·áµ¢ âˆ’ yáµ¢)

Where:
- Î±: learning rate

---

##  9. Final Objective Function

J(Î¸) = âˆ’(1/m) âˆ‘ [yáµ¢ log(Å·áµ¢) + (1 âˆ’ yáµ¢) log(1 âˆ’ Å·áµ¢)] + (Î» / 2m) âˆ‘ Î¸â±¼Â²

---

#  10. Summary
- Logistic regression predicts P(y = 1 | x) using the sigmoid of a linear combination.
- Cross-entropy loss penalizes incorrect probabilities.
- L2 regularization keeps weights small to reduce overfitting.
- Model is trained using gradient descent with regularized gradients.
- The final model is simple, robust, and interpretable.

Youâ€™ve now fully derived logistic regression with L2 regularization â€” end to end!
