Imbalanced Data – Problems and Solutions

1. Problems with Imbalanced Data
--------------------------------
1.1 Bias in Predictions
    - The model becomes biased toward the majority class.
    - A trivial classifier that always predicts the majority class may have high accuracy but zero utility for the minority class.

1.2 Metrics Are Not Reliable
    - Accuracy is misleading (e.g., 95% accuracy by predicting all zeros when minority class is only 5%).
    - Need metrics like Precision, Recall, F1-score, ROC-AUC, or PR-AUC.


2. Methods to Handle Imbalanced Data
------------------------------------

2.1 Undersampling the Majority Class
------------------------------------
- Definition: Randomly remove samples from the majority class to balance the dataset.

Advantages:
    - Reduces training time (smaller dataset).
    - Simple and easy to implement.
    - Works well when the majority class is very large and redundant.

Disadvantages:
    - Risk of removing important samples → loss of information.
    - May cause underfitting if too many samples are removed.

When to Use:
    - When the majority class size is huge and you can afford to discard data.
    - When training time is a concern.

---

2.2 Oversampling the Minority Class
------------------------------------
- Definition: Duplicate or replicate minority class samples to match the majority class.

Advantages:
    - No data loss from majority class.
    - Helps the model learn minority patterns better.

Disadvantages:
    - Risk of overfitting since minority class samples are repeated.
    - Increases training time.

When to Use:
    - When the dataset is small and you cannot afford to discard majority samples.
    - When the minority class is extremely underrepresented.

---

2.3 SMOTE (Synthetic Minority Oversampling Technique)
------------------------------------------------------
- Definition: Generates synthetic samples for the minority class by interpolating between existing samples and their nearest neighbors.
  Formula:
    new_sample = sample + factor * (neighbor - sample)
  where factor ∈ [0, 1].

Advantages:
    - Reduces overfitting compared to naive oversampling.
    - Creates new, diverse synthetic samples instead of duplicates.
    - Improves minority class representation.

Disadvantages:
    - Can create overlapping between classes, especially if classes are not well separated.
    - May generate noisy or unrealistic samples.
    - Not suitable for categorical features (requires SMOTENC).

When to Use:
    - When the dataset is small but minority patterns can be reasonably interpolated.
    - When overfitting is observed with naive oversampling.

Always perform resampling only on training data to prevent data leakage.


---

2.4 Ensemble Methods
---------------------
- Definition: Combine multiple models to improve robustness (e.g., Balanced Random Forest, EasyEnsemble, XGBoost with class weights).

Advantages:
    - Reduces bias by combining multiple learners.
    - Can handle imbalance better when combined with resampling.
    - Often achieves state-of-the-art performance.

Disadvantages:
    - Increased computational cost.
    - Harder to interpret compared to a single model.

When to Use:
    - When you need high performance and can afford the computational cost.
    - When simpler methods fail to handle imbalance effectively.

---

2.5 Cost-Sensitive Learning
----------------------------
- Definition: Assign different misclassification costs or class weights so that errors on the minority class are penalized more.

5.1 Class Weight Assignment:
    - Example (Scikit-learn):
      LogisticRegression(class_weight='balanced')

Advantages:
    - No need to modify data (no oversampling/undersampling).
    - Easy to implement (many libraries support class weights).
    - Works well with linear models and tree-based methods.

Disadvantages:
    - Finding optimal class weights can be tricky.
    - May still struggle when data imbalance is extreme.

When to Use:
    - As a first approach (easy and quick).
    - When data volume is large and resampling is expensive.

---

2.6 --

Threshold Tuning

Logistic regression outputs probabilities p = σ(wᵗx).

The default classification threshold = 0.5.

For imbalanced data, lowering the threshold (e.g., 0.2) can improve recall for the minority class.

How to find optimal threshold:

Use ROC or Precision-Recall curves.

Select threshold that maximizes F1-score or balanced accuracy.


3. When to Use Which Method?
----------------------------
- Start with class weights (cost-sensitive learning) – simple and effective for many models.
- If the dataset is small → SMOTE or oversampling can help.
- If the dataset is huge → undersampling or balanced ensembles might be better.
- For complex models or high performance → Combine SMOTE + Ensemble or use advanced techniques like Balanced Random Forest** or **XGBoost with scale_pos_weight**.

---

4. Key Takeaways
----------------
- No single method works best in all cases; choose based on:
  - Data size
  - Minority class importance
  - Risk of overfitting
  - Computational constraints
- Always use recall, precision, F1, and PR-AUC to evaluate performance on imbalanced datasets.
